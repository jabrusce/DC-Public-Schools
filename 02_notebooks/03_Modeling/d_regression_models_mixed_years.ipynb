{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models (Mixed Years)\n",
    "* looks at mixed years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from category_encoders import OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../01_data/cleaned_data/school_df_v6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 844 entries, 0 to 843\n",
      "Data columns (total 56 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   code                         109 non-null    float64\n",
      " 1   name                         109 non-null    object \n",
      " 2   grade_band                   109 non-null    object \n",
      " 3   enrollment_SY1718            109 non-null    float64\n",
      " 4   enrollment_SY1819            109 non-null    float64\n",
      " 5   star_score_SY1718            109 non-null    float64\n",
      " 6   star_score_SY1819            109 non-null    float64\n",
      " 7   star_rating_SY1718           109 non-null    float64\n",
      " 8   star_rating_SY1819           109 non-null    float64\n",
      " 9   capacity_SY1718              109 non-null    float64\n",
      " 10  capacity_SY1819              109 non-null    float64\n",
      " 11  latitude                     109 non-null    float64\n",
      " 12  longitude                    109 non-null    float64\n",
      " 13  cluster                      109 non-null    float64\n",
      " 14  ward                         109 non-null    float64\n",
      " 15  count_any_SY1819             109 non-null    float64\n",
      " 16  count_0_SY1819               109 non-null    float64\n",
      " 17  pct_0_SY1819                 109 non-null    float64\n",
      " 18  count_1-5_SY1819             107 non-null    float64\n",
      " 19  pct_1-5_SY1819               109 non-null    float64\n",
      " 20  count_6-10_SY1819            107 non-null    float64\n",
      " 21  pct_6-10_SY1819              109 non-null    float64\n",
      " 22  count_11-20_SY1819           107 non-null    float64\n",
      " 23  pct_11-20_SY1819             109 non-null    float64\n",
      " 24  count_20+_SY1819             108 non-null    float64\n",
      " 25  pct_20+_SY1819               109 non-null    float64\n",
      " 26  count_any_SY1718             109 non-null    float64\n",
      " 27  count_0_SY1718               109 non-null    float64\n",
      " 28  pct_0_SY1718                 109 non-null    float64\n",
      " 29  count_1-5_SY1718             107 non-null    float64\n",
      " 30  pct_1-5_SY1718               109 non-null    float64\n",
      " 31  count_6-10_SY1718            107 non-null    float64\n",
      " 32  pct_6-10__SY1718             109 non-null    float64\n",
      " 33  count_11-20_SY1718           107 non-null    float64\n",
      " 34  pct_11-20_SY1718             109 non-null    float64\n",
      " 35  count_20+_SY1718             107 non-null    float64\n",
      " 36  pct_20+_SY1718               109 non-null    float64\n",
      " 37  budgeted_amount_FY16         109 non-null    float64\n",
      " 38  budgeted_enrollment_FY16     109 non-null    float64\n",
      " 39  budgeted_amount_FY17         109 non-null    float64\n",
      " 40  budgeted_enrollment_FY17     109 non-null    float64\n",
      " 41  pct_meet_exceed_math_SY1718  109 non-null    float64\n",
      " 42  pct_meet_exceed_ela_SY1718   109 non-null    float64\n",
      " 43  pct_meet_exceed_math_SY1819  109 non-null    float64\n",
      " 44  pct_meet_exceed_ela_SY1819   109 non-null    float64\n",
      " 45  Unnamed: 45                  0 non-null      float64\n",
      " 46  Unnamed: 46                  0 non-null      float64\n",
      " 47  Unnamed: 47                  0 non-null      float64\n",
      " 48  Unnamed: 48                  0 non-null      float64\n",
      " 49  Unnamed: 49                  0 non-null      float64\n",
      " 50  Unnamed: 50                  0 non-null      float64\n",
      " 51  Unnamed: 51                  0 non-null      float64\n",
      " 52  Unnamed: 52                  0 non-null      float64\n",
      " 53  Unnamed: 53                  0 non-null      float64\n",
      " 54  Unnamed: 54                  0 non-null      float64\n",
      " 55  Unnamed: 55                  0 non-null      float64\n",
      "dtypes: float64(54), object(2)\n",
      "memory usage: 369.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnamed columns and \"bad import\" full-nan rows\n",
    "data = data.iloc[:,:45]\n",
    "data.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 2 schools that have no attendance data\n",
    "data.drop(data[data['code'] == 201].index, inplace = True) # Oyster Adams Bilingual School (Adams) has no attendance data\n",
    "data.drop(data[data['code'] == 347].index, inplace = True) # Brookland Middle School has no attendance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns looking at \"count\" of absences (non-relatable across schools of different sizes)\n",
    "X = data.drop(columns = data.filter(regex='^count',axis=1)).drop(columns='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop school_code\n",
    "X.drop('code', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    }
   ],
   "source": [
    "# OHE grade_band column\n",
    "X_ohe = OneHotEncoder(cols=['grade_band']).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# Create no_nan_df\n",
    "### Only looks at 82 rows\n",
    "* Need a better solution for dealing with np.nan (originally cast as -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nan_df = X_ohe.replace(-1,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nan_df.dropna(how='any',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nan_y = no_nan_df['star_rating_SY1718']\n",
    "no_nan_X = no_nan_df.drop(columns=['star_score_SY1718','star_score_SY1819','star_rating_SY1718','star_rating_SY1819'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(no_nan_X, no_nan_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "# Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(grid, X_train, X_test, y_train, y_test):\n",
    "    #print(f'Estimator : {grid.get_estimator_}')\n",
    "    \n",
    "    print('*** TRAIN set ***')\n",
    "    print(f'Score : {grid.score(X_train, y_train)}')\n",
    "    print(f'RMSE: {mean_squared_error(y_train, grid.predict(X_train), squared = False)}')\n",
    "    print(f'MSE: {mean_squared_error(y_train, grid.predict(X_train))}')\n",
    "    print()\n",
    "    print('*** TEST set ***')\n",
    "    print(f'Score : {grid.score(X_test, y_test)}')\n",
    "    print(f'RMSE: {mean_squared_error(y_test, grid.predict(X_test), squared = False)}')\n",
    "    print(f'MSE: {mean_squared_error(y_test, grid.predict(X_test))}')\n",
    "    print()\n",
    "    print(f'Best Params : {grid.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TRAIN set ***\n",
      "Score : 1.0\n",
      "RMSE: 4.275652566708103e-15\n",
      "MSE: 1.8281204871197588e-29\n",
      "\n",
      "*** TEST set ***\n",
      "Score : -0.9191336230398051\n",
      "RMSE: 1.671478622226844\n",
      "MSE: 2.7938407845613487\n",
      "\n",
      "Best Params : {'lr__normalize': 'True'}\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('poly', PolynomialFeatures()), ('scaler', StandardScaler()), ('lr', LinearRegression())])\n",
    "param = [\n",
    "    {'lr__normalize': ['True','False']}\n",
    "]\n",
    "grid = GridSearchCV(pipe, param, n_jobs = -1)\n",
    "grid.fit(X_train, y_train)\n",
    "print_results(grid, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TRAIN set ***\n",
      "Score : 0.8621880579061941\n",
      "RMSE: 0.42242244827577313\n",
      "MSE: 0.17844072480729822\n",
      "\n",
      "*** TEST set ***\n",
      "Score : 0.6275837159956574\n",
      "RMSE: 0.7363131394310704\n",
      "MSE: 0.5421570392988388\n",
      "\n",
      "Best Params : {'ridge__alpha': 100}\n"
     ]
    }
   ],
   "source": [
    "pipe1 = Pipeline([('poly', PolynomialFeatures()), ('scaler', StandardScaler()), ('ridge', Ridge())])\n",
    "param1 = [\n",
    "    {'ridge__alpha': [.01, .1, 1, 10, 100]}\n",
    "]\n",
    "grid1 = GridSearchCV(pipe1, param1, n_jobs = -1)\n",
    "grid1.fit(X_train, y_train)\n",
    "print_results(grid1, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighbors Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TRAIN set ***\n",
      "Score : 0.634404728823386\n",
      "RMSE: 0.6880244117124881\n",
      "MSE: 0.47337759111231537\n",
      "\n",
      "*** TEST set ***\n",
      "Score : 0.5828763420097318\n",
      "RMSE: 0.7792568534218228\n",
      "MSE: 0.6072412436048802\n",
      "\n",
      "Best Params : {'knearest__n_neighbors': 11}\n"
     ]
    }
   ],
   "source": [
    "pipe2 = Pipeline([('poly', PolynomialFeatures()), ('scaler', StandardScaler()), ('knearest', KNeighborsRegressor())])\n",
    "param2 = [\n",
    "    {'knearest__n_neighbors': [3,7,11,25,35]}\n",
    "]\n",
    "grid2 = GridSearchCV(pipe2, param2, n_jobs = -1)\n",
    "grid2.fit(X_train, y_train)\n",
    "print_results(grid2, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TRAIN set ***\n",
      "Score : 0.9375196847799587\n",
      "RMSE: 0.28442984772591545\n",
      "MSE: 0.08090033827738745\n",
      "\n",
      "*** TEST set ***\n",
      "Score : 0.4246911931958661\n",
      "RMSE: 0.9151635839649335\n",
      "MSE: 0.8375243854155417\n",
      "\n",
      "Best Params : {'dtree__ccp_alpha': 0.01, 'dtree__max_depth': 9, 'dtree__min_samples_leaf': 5, 'dtree__min_samples_split': 9}\n"
     ]
    }
   ],
   "source": [
    "pipe3 = Pipeline([('poly', PolynomialFeatures()), ('scaler', StandardScaler()), ('dtree', DecisionTreeRegressor())])\n",
    "param3 = [\n",
    "    {'dtree__max_depth':[3,5,7,9],\n",
    "    'dtree__min_samples_split':[3,5,7,9], \n",
    "    'dtree__min_samples_leaf':[3,5,7,9], \n",
    "    'dtree__ccp_alpha':[.01, .1, 1, 10, 100]}\n",
    "]\n",
    "grid3 = GridSearchCV(pipe3, param3, n_jobs = -1)\n",
    "grid3.fit(X_train, y_train)\n",
    "print_results(grid3, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagged Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TRAIN set ***\n",
      "Score : 0.9614654213366542\n",
      "RMSE: 0.22337207069643492\n",
      "MSE: 0.04989508196721312\n",
      "\n",
      "*** TEST set ***\n",
      "Score : 0.5157733644859812\n",
      "RMSE: 0.8396002450145971\n",
      "MSE: 0.7049285714285715\n",
      "\n",
      "Best Params : {'btree__n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "pipe4 = Pipeline([('poly', PolynomialFeatures()), ('scaler', StandardScaler()), ('btree', BaggingRegressor())])\n",
    "param4 = [\n",
    "    {'btree__n_estimators':[50, 100, 500]}\n",
    "]\n",
    "grid4 = GridSearchCV(pipe4, param4, n_jobs = -1)\n",
    "grid4.fit(X_train, y_train)\n",
    "print_results(grid4, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe5 = Pipeline([('poly', PolynomialFeatures()), ('scaler', StandardScaler()), ('randforest', RandomForestRegressor())])\n",
    "param5 = [\n",
    "    {'randforest__max_depth':[3,5,7,9],\n",
    "    'randforest__min_samples_split':[3,5,7,9], \n",
    "    'randforest__min_samples_leaf':[3,5,7,9], \n",
    "    'randforest__n_estimators':[50, 100, 500]}\n",
    "]\n",
    "grid5 = GridSearchCV(pipe5, param5, n_jobs = -1)\n",
    "grid5.fit(X_train, y_train)\n",
    "print_results(grid5, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe6 = Pipeline([('poly', PolynomialFeatures()), ('scaler', StandardScaler()), ('ada', AdaBoostRegressor())])\n",
    "param6 = [\n",
    "    {'ada__n_estimators':[50, 100, 500]}\n",
    "]\n",
    "grid6 = GridSearchCV(pipe6, param6, n_jobs = -1)\n",
    "grid6.fit(X_train, y_train)\n",
    "print_results(grid6, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regressor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Was only able to get a very limited SVM to ever execute fully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe7 = Pipeline([('poly', PolynomialFeatures()), ('scaler', StandardScaler()), ('svr', SVR())])\n",
    "param7 = [\n",
    "    {'svr__kernel': ['linear'],\n",
    "     'svr__C': [.01, 1, 10],\n",
    "     'svr__gamma': np.logspace(-2, 2, 20)\n",
    "    }\n",
    "#     {'svr__kernel': ['linear', 'poly'],\n",
    "#      'svr__C': [.01, 1, 100],\n",
    "#      'svr__gamma': np.logspace(-5, 2, 20)\n",
    "#     }\n",
    "]\n",
    "grid7 = GridSearchCV(pipe7, param7, n_jobs = -1)\n",
    "grid7.fit(X_train, y_train)\n",
    "print_results(grid7, X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
